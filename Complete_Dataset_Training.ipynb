{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41539800",
   "metadata": {},
   "source": [
    "# üöÄ Complete Dataset Training - Advanced Question Generation AI\n",
    "\n",
    "Notebook n√†y s·∫Ω train to√†n b·ªô d·ªØ li·ªáu t·ª´:\n",
    "- **datasets/**: H√†ng trƒÉm file CSV v·ªõi form data \n",
    "- **question_datasets/**: H√†ng trƒÉm file CSV v·ªõi real questions\n",
    "\n",
    "M·ª•c ti√™u: T·∫°o AI model m·∫°nh m·∫Ω c√≥ th·ªÉ generate c√¢u h·ªèi th√¥ng minh t·ª´ keywords!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f02cc1",
   "metadata": {},
   "source": [
    "## üìö B∆∞·ªõc 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Text Processing\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Training started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a674c7bc",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è B∆∞·ªõc 2: Set Up Data Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d7bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "BASE_PATH = Path(\"/home/dtu/ form-agent-AI-project/form-agent-AI-project\")\n",
    "DATASETS_PATH = BASE_PATH / \"datasets\"\n",
    "QUESTION_DATASETS_PATH = BASE_PATH / \"question_datasets\"\n",
    "MODELS_PATH = BASE_PATH / \"models\"\n",
    "\n",
    "# Create models directory if not exists\n",
    "MODELS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'max_samples_per_batch': 50000,  # Gi·ªõi h·∫°n samples per file ƒë·ªÉ tr√°nh memory overflow\n",
    "    'total_max_samples': 1000000,    # T·ªïng s·ªë samples t·ªëi ƒëa\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'min_question_length': 10,\n",
    "    'max_question_length': 200,\n",
    "    'categories': ['it', 'economics', 'marketing']\n",
    "}\n",
    "\n",
    "print(f\"üìÇ Dataset path: {DATASETS_PATH}\")\n",
    "print(f\"üìÇ Question datasets path: {QUESTION_DATASETS_PATH}\")\n",
    "print(f\"üíæ Models will be saved to: {MODELS_PATH}\")\n",
    "print(f\"‚öôÔ∏è Configuration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af15274",
   "metadata": {},
   "source": [
    "## üîç B∆∞·ªõc 3: Load and Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f387f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files in both directories\n",
    "dataset_files = list(DATASETS_PATH.glob(\"*.csv\"))\n",
    "question_files = list(QUESTION_DATASETS_PATH.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"üìä Found {len(dataset_files)} files in datasets/\")\n",
    "print(f\"üìä Found {len(question_files)} files in question_datasets/\")\n",
    "\n",
    "# Load sample files to understand structure\n",
    "if dataset_files:\n",
    "    sample_dataset = pd.read_csv(dataset_files[0])\n",
    "    print(f\"\\nüîç Sample from datasets/ ({dataset_files[0].name}):\")\n",
    "    print(f\"   Shape: {sample_dataset.shape}\")\n",
    "    print(f\"   Columns: {list(sample_dataset.columns)}\")\n",
    "    display(sample_dataset.head(3))\n",
    "\n",
    "if question_files:\n",
    "    sample_questions = pd.read_csv(question_files[0])\n",
    "    print(f\"\\nüîç Sample from question_datasets/ ({question_files[0].name}):\")\n",
    "    print(f\"   Shape: {sample_questions.shape}\")\n",
    "    print(f\"   Columns: {list(sample_questions.columns)}\")\n",
    "    display(sample_questions.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c58573",
   "metadata": {},
   "source": [
    "## üßπ B∆∞·ªõc 4: Data Preprocessing and Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text data\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower().strip()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep Vietnamese\n",
    "    text = re.sub(r'[^\\w\\s\\u00C0-\\u1EF9\\?\\.]', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_questions_from_form_data(df):\n",
    "    \"\"\"Extract questions from form data format\"\"\"\n",
    "    questions_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            # Assume form data has columns like 'field_name', 'category', etc.\n",
    "            if 'category' in row and 'field_name' in row:\n",
    "                category = str(row['category']).lower()\n",
    "                field_name = clean_text(row['field_name'])\n",
    "                \n",
    "                # Generate questions from field names\n",
    "                if field_name and len(field_name) > 5:\n",
    "                    question_templates = [\n",
    "                        f\"What is {field_name}?\",\n",
    "                        f\"How do you define {field_name}?\",\n",
    "                        f\"What are the key aspects of {field_name}?\"\n",
    "                    ]\n",
    "                    \n",
    "                    for template in question_templates:\n",
    "                        questions_data.append({\n",
    "                            'question': template,\n",
    "                            'keyword': field_name,\n",
    "                            'category': category,\n",
    "                            'source': 'form_data'\n",
    "                        })\n",
    "                        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(questions_data)\n",
    "\n",
    "def process_question_data(df):\n",
    "    \"\"\"Process direct question data\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            question = clean_text(row.get('question', ''))\n",
    "            keyword = clean_text(row.get('keyword', ''))\n",
    "            category = str(row.get('category', 'it')).lower()\n",
    "            \n",
    "            # Validate data\n",
    "            if (question and keyword and \n",
    "                len(question) >= CONFIG['min_question_length'] and \n",
    "                len(question) <= CONFIG['max_question_length']):\n",
    "                \n",
    "                processed_data.append({\n",
    "                    'question': question,\n",
    "                    'keyword': keyword,\n",
    "                    'category': category,\n",
    "                    'source': 'direct_questions'\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "print(\"‚úÖ Data preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fb1ce",
   "metadata": {},
   "source": [
    "## üìÇ B∆∞·ªõc 5: Load All CSV Files - Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_datasets():\n",
    "    \"\"\"Load and combine all CSV files from both directories\"\"\"\n",
    "    all_data = []\n",
    "    total_loaded = 0\n",
    "    \n",
    "    print(\"üöÄ Loading datasets from datasets/ folder...\")\n",
    "    \n",
    "    # Process form datasets\n",
    "    for i, file_path in enumerate(tqdm(dataset_files[:20])):  # Limit to first 20 files for demo\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Limit samples per file\n",
    "            if len(df) > CONFIG['max_samples_per_batch']:\n",
    "                df = df.sample(n=CONFIG['max_samples_per_batch'], random_state=42)\n",
    "            \n",
    "            # Extract questions from form data\n",
    "            processed_df = extract_questions_from_form_data(df)\n",
    "            \n",
    "            if not processed_df.empty:\n",
    "                all_data.append(processed_df)\n",
    "                total_loaded += len(processed_df)\n",
    "                \n",
    "            print(f\"   ‚úÖ {file_path.name}: {len(processed_df):,} questions extracted\")\n",
    "            \n",
    "            # Stop if we've reached max samples\n",
    "            if total_loaded >= CONFIG['total_max_samples']:\n",
    "                print(f\"   üõë Reached maximum samples limit: {CONFIG['total_max_samples']:,}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {file_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüöÄ Loading question datasets from question_datasets/ folder...\")\n",
    "    \n",
    "    # Process direct question datasets\n",
    "    for i, file_path in enumerate(tqdm(question_files[:10])):  # Limit to first 10 files\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Limit samples per file\n",
    "            if len(df) > CONFIG['max_samples_per_batch']:\n",
    "                df = df.sample(n=CONFIG['max_samples_per_batch'], random_state=42)\n",
    "            \n",
    "            # Process question data\n",
    "            processed_df = process_question_data(df)\n",
    "            \n",
    "            if not processed_df.empty:\n",
    "                all_data.append(processed_df)\n",
    "                total_loaded += len(processed_df)\n",
    "                \n",
    "            print(f\"   ‚úÖ {file_path.name}: {len(processed_df):,} questions loaded\")\n",
    "            \n",
    "            # Stop if we've reached max samples\n",
    "            if total_loaded >= CONFIG['total_max_samples']:\n",
    "                print(f\"   üõë Reached maximum samples limit: {CONFIG['total_max_samples']:,}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {file_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\n‚úÖ Total combined data: {len(combined_df):,} records\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"‚ùå No data loaded!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load all data\n",
    "print(\"üöÄ Starting comprehensive data loading...\")\n",
    "start_time = time.time()\n",
    "\n",
    "master_dataset = load_all_datasets()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n‚è±Ô∏è Data loading completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if not master_dataset.empty:\n",
    "    print(f\"\\nüìä Master Dataset Summary:\")\n",
    "    print(f\"   Total records: {len(master_dataset):,}\")\n",
    "    print(f\"   Unique questions: {master_dataset['question'].nunique():,}\")\n",
    "    print(f\"   Unique keywords: {master_dataset['keyword'].nunique():,}\")\n",
    "    print(f\"   Categories: {master_dataset['category'].value_counts().to_dict()}\")\n",
    "    print(f\"   Sources: {master_dataset['source'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7600d88",
   "metadata": {},
   "source": [
    "## üîß B∆∞·ªõc 6: Feature Engineering and Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and clean data\n",
    "print(\"üßπ Cleaning and deduplicating data...\")\n",
    "initial_size = len(master_dataset)\n",
    "\n",
    "# Remove duplicates based on question content\n",
    "master_dataset = master_dataset.drop_duplicates(subset=['question'], keep='first')\n",
    "print(f\"   Removed {initial_size - len(master_dataset):,} duplicate questions\")\n",
    "\n",
    "# Filter valid categories\n",
    "master_dataset = master_dataset[master_dataset['category'].isin(CONFIG['categories'])]\n",
    "print(f\"   Filtered to valid categories: {len(master_dataset):,} records\")\n",
    "\n",
    "# Create features for machine learning\n",
    "print(\"\\nüîß Creating features for ML training...\")\n",
    "\n",
    "# Text features from questions\n",
    "master_dataset['question_length'] = master_dataset['question'].str.len()\n",
    "master_dataset['word_count'] = master_dataset['question'].str.split().str.len()\n",
    "master_dataset['keyword_length'] = master_dataset['keyword'].str.len()\n",
    "\n",
    "# Encode categories\n",
    "label_encoder = LabelEncoder()\n",
    "master_dataset['category_encoded'] = label_encoder.fit_transform(master_dataset['category'])\n",
    "\n",
    "print(f\"‚úÖ Feature engineering completed!\")\n",
    "print(f\"   Question length stats: {master_dataset['question_length'].describe()}\")\n",
    "print(f\"   Word count stats: {master_dataset['word_count'].describe()}\")\n",
    "\n",
    "# Display final dataset info\n",
    "display(master_dataset.head())\n",
    "print(f\"\\nüìä Final dataset shape: {master_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceb3e8",
   "metadata": {},
   "source": [
    "## üìä B∆∞·ªõc 7: Data Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Category distribution\n",
    "master_dataset['category'].value_counts().plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Distribution of Categories')\n",
    "axes[0,0].set_xlabel('Category')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# Question length distribution\n",
    "master_dataset['question_length'].hist(bins=50, ax=axes[0,1], color='lightgreen')\n",
    "axes[0,1].set_title('Distribution of Question Lengths')\n",
    "axes[0,1].set_xlabel('Question Length (characters)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Word count distribution\n",
    "master_dataset['word_count'].hist(bins=30, ax=axes[1,0], color='orange')\n",
    "axes[1,0].set_title('Distribution of Word Count per Question')\n",
    "axes[1,0].set_xlabel('Word Count')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Source distribution\n",
    "master_dataset['source'].value_counts().plot(kind='pie', ax=axes[1,1], autopct='%1.1f%%')\n",
    "axes[1,1].set_title('Distribution of Data Sources')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\nüìà Detailed Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "for category in CONFIG['categories']:\n",
    "    cat_data = master_dataset[master_dataset['category'] == category]\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    print(f\"   Records: {len(cat_data):,}\")\n",
    "    print(f\"   Avg question length: {cat_data['question_length'].mean():.1f}\")\n",
    "    print(f\"   Avg word count: {cat_data['word_count'].mean():.1f}\")\n",
    "    print(f\"   Unique keywords: {cat_data['keyword'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d4192",
   "metadata": {},
   "source": [
    "## üîÄ B∆∞·ªõc 8: Prepare Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and targets\n",
    "print(\"üîÄ Splitting data into training and validation sets...\")\n",
    "\n",
    "# Features: use keywords and questions\n",
    "X = master_dataset[['keyword', 'question']]\n",
    "y = master_dataset['category']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=CONFIG['test_size'], \n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data split completed:\")\n",
    "print(f\"   Training set: {len(X_train):,} samples\")\n",
    "print(f\"   Test set: {len(X_test):,} samples\")\n",
    "print(f\"   Training categories: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"   Test categories: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Create TF-IDF features for keywords and questions\n",
    "print(\"\\nüîß Creating TF-IDF features...\")\n",
    "\n",
    "# TF-IDF for keywords\n",
    "keyword_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# TF-IDF for questions\n",
    "question_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_keyword_tfidf = keyword_vectorizer.fit_transform(X_train['keyword'])\n",
    "X_train_question_tfidf = question_vectorizer.fit_transform(X_train['question'])\n",
    "\n",
    "X_test_keyword_tfidf = keyword_vectorizer.transform(X_test['keyword'])\n",
    "X_test_question_tfidf = question_vectorizer.transform(X_test['question'])\n",
    "\n",
    "print(f\"   Keyword TF-IDF shape: {X_train_keyword_tfidf.shape}\")\n",
    "print(f\"   Question TF-IDF shape: {X_train_question_tfidf.shape}\")\n",
    "\n",
    "# Combine features\n",
    "from scipy.sparse import hstack\n",
    "X_train_combined = hstack([X_train_keyword_tfidf, X_train_question_tfidf])\n",
    "X_test_combined = hstack([X_test_keyword_tfidf, X_test_question_tfidf])\n",
    "\n",
    "print(f\"   Combined features shape: {X_train_combined.shape}\")\n",
    "print(\"‚úÖ Feature preparation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b37b92",
   "metadata": {},
   "source": [
    "## ü§ñ B∆∞·ªõc 9: Model Architecture Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple models for comparison\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"ü§ñ Training multiple models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trained_models = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüî• Training {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_combined, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_combined)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[model_name] = model\n",
    "    model_results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ {model_name} completed!\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(model_results, key=lambda x: model_results[x]['accuracy'])\n",
    "best_model = trained_models[best_model_name]\n",
    "best_accuracy = model_results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"üéØ Best Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee9178",
   "metadata": {},
   "source": [
    "## üìä B∆∞·ªõc 10: Model Evaluation and Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad132f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of the best model\n",
    "print(f\"üìä Detailed Evaluation of {best_model_name}:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=CONFIG['categories'], \n",
    "            yticklabels=CONFIG['categories'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Model comparison visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "model_names = list(model_results.keys())\n",
    "accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Training time comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "training_times = [model_results[name]['training_time'] for name in model_names]\n",
    "plt.bar(model_names, training_times, color=['orange', 'pink'])\n",
    "plt.title('Training Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance by category\n",
    "print(\"\\nüìà Performance by Category:\")\n",
    "for category in CONFIG['categories']:\n",
    "    category_mask = y_test == category\n",
    "    category_accuracy = accuracy_score(y_test[category_mask], best_predictions[category_mask])\n",
    "    category_count = category_mask.sum()\n",
    "    print(f\"   {category.upper()}: {category_accuracy:.4f} (n={category_count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a855c6",
   "metadata": {},
   "source": [
    "## üîÆ B∆∞·ªõc 11: Create Question Generation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive question generation system\n",
    "class AdvancedQuestionGenerator:\n",
    "    def __init__(self, classifier, keyword_vectorizer, question_vectorizer, \n",
    "                 training_data, label_encoder):\n",
    "        self.classifier = classifier\n",
    "        self.keyword_vectorizer = keyword_vectorizer\n",
    "        self.question_vectorizer = question_vectorizer\n",
    "        self.training_data = training_data\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        # Build similarity index\n",
    "        self.similarity_model = NearestNeighbors(\n",
    "            n_neighbors=10, \n",
    "            metric='cosine'\n",
    "        )\n",
    "        \n",
    "        # Fit on training keywords\n",
    "        training_keyword_tfidf = keyword_vectorizer.transform(training_data['keyword'])\n",
    "        self.similarity_model.fit(training_keyword_tfidf)\n",
    "    \n",
    "    def predict_category(self, keyword):\n",
    "        \"\"\"Predict category for a keyword\"\"\"\n",
    "        # Create dummy question for prediction\n",
    "        dummy_question = f\"What is {keyword}?\"\n",
    "        \n",
    "        # Vectorize\n",
    "        keyword_tfidf = self.keyword_vectorizer.transform([keyword])\n",
    "        question_tfidf = self.question_vectorizer.transform([dummy_question])\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = hstack([keyword_tfidf, question_tfidf])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.classifier.predict(combined_features)[0]\n",
    "        probabilities = self.classifier.predict_proba(combined_features)[0]\n",
    "        confidence = max(probabilities)\n",
    "        \n",
    "        return prediction, confidence\n",
    "    \n",
    "    def find_similar_keywords(self, keyword, n_similar=5):\n",
    "        \"\"\"Find similar keywords from training data\"\"\"\n",
    "        keyword_tfidf = self.keyword_vectorizer.transform([keyword])\n",
    "        distances, indices = self.similarity_model.kneighbors(keyword_tfidf)\n",
    "        \n",
    "        similar_data = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            similar_row = self.training_data.iloc[idx]\n",
    "            similar_data.append({\n",
    "                'keyword': similar_row['keyword'],\n",
    "                'question': similar_row['question'],\n",
    "                'category': similar_row['category'],\n",
    "                'similarity': 1 - distances[0][i]\n",
    "            })\n",
    "        \n",
    "        return similar_data\n",
    "    \n",
    "    def generate_questions(self, keyword, num_questions=5):\n",
    "        \"\"\"Generate questions for a keyword using ML similarity\"\"\"\n",
    "        # Predict category\n",
    "        predicted_category, confidence = self.predict_category(keyword)\n",
    "        \n",
    "        # Find similar keywords and their questions\n",
    "        similar_data = self.find_similar_keywords(keyword, n_similar=10)\n",
    "        \n",
    "        generated_questions = []\n",
    "        \n",
    "        # Use similar questions as templates\n",
    "        for item in similar_data[:num_questions]:\n",
    "            original_question = item['question']\n",
    "            \n",
    "            # Adapt question by replacing similar keyword with target keyword\n",
    "            adapted_question = self.adapt_question(original_question, item['keyword'], keyword)\n",
    "            \n",
    "            generated_questions.append({\n",
    "                'question': adapted_question,\n",
    "                'category': predicted_category,\n",
    "                'confidence': confidence,\n",
    "                'similarity': item['similarity'],\n",
    "                'source': 'ml_similarity'\n",
    "            })\n",
    "        \n",
    "        # If we need more questions, generate template-based ones\n",
    "        if len(generated_questions) < num_questions:\n",
    "            template_questions = self.generate_template_questions(keyword, predicted_category)\n",
    "            \n",
    "            for template_q in template_questions[:num_questions - len(generated_questions)]:\n",
    "                generated_questions.append({\n",
    "                    'question': template_q,\n",
    "                    'category': predicted_category,\n",
    "                    'confidence': confidence,\n",
    "                    'similarity': 0.5,\n",
    "                    'source': 'template'\n",
    "                })\n",
    "        \n",
    "        return generated_questions[:num_questions]\n",
    "    \n",
    "    def adapt_question(self, original_question, original_keyword, target_keyword):\n",
    "        \"\"\"Adapt a question by replacing keywords intelligently\"\"\"\n",
    "        adapted = original_question.replace(original_keyword.lower(), target_keyword.lower())\n",
    "        \n",
    "        # Capitalize first letter\n",
    "        adapted = adapted[0].upper() + adapted[1:] if len(adapted) > 1 else adapted.upper()\n",
    "        \n",
    "        return adapted\n",
    "    \n",
    "    def generate_template_questions(self, keyword, category):\n",
    "        \"\"\"Generate template-based questions as fallback\"\"\"\n",
    "        templates = {\n",
    "            'it': [\n",
    "                f\"What is {keyword}?\",\n",
    "                f\"How does {keyword} work?\",\n",
    "                f\"What are the benefits of {keyword}?\",\n",
    "                f\"How to implement {keyword}?\",\n",
    "                f\"What are the best practices for {keyword}?\"\n",
    "            ],\n",
    "            'economics': [\n",
    "                f\"What is {keyword}?\",\n",
    "                f\"How does {keyword} affect the economy?\",\n",
    "                f\"What are the economic implications of {keyword}?\",\n",
    "                f\"How to analyze {keyword}?\",\n",
    "                f\"What strategies work for {keyword}?\"\n",
    "            ],\n",
    "            'marketing': [\n",
    "                f\"What is {keyword}?\",\n",
    "                f\"How to use {keyword} in marketing?\",\n",
    "                f\"What are the best {keyword} strategies?\",\n",
    "                f\"How to measure {keyword} effectiveness?\",\n",
    "                f\"What tools help with {keyword}?\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return templates.get(category, templates['it'])\n",
    "\n",
    "# Initialize the question generator\n",
    "print(\"üîÆ Creating Advanced Question Generator...\")\n",
    "question_generator = AdvancedQuestionGenerator(\n",
    "    classifier=best_model,\n",
    "    keyword_vectorizer=keyword_vectorizer,\n",
    "    question_vectorizer=question_vectorizer,\n",
    "    training_data=master_dataset,\n",
    "    label_encoder=label_encoder\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Question Generator created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd89f84",
   "metadata": {},
   "source": [
    "## üß™ B∆∞·ªõc 12: Test Question Generation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f0226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the question generation system\n",
    "print(\"üß™ Testing Advanced Question Generation System\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_keywords = [\n",
    "    \"artificial intelligence\",\n",
    "    \"blockchain technology\",\n",
    "    \"cryptocurrency investment\",\n",
    "    \"digital marketing automation\",\n",
    "    \"cloud computing security\",\n",
    "    \"machine learning algorithms\",\n",
    "    \"social media advertising\",\n",
    "    \"financial portfolio management\",\n",
    "    \"data science\",\n",
    "    \"e-commerce optimization\"\n",
    "]\n",
    "\n",
    "for keyword in test_keywords:\n",
    "    print(f\"\\nüéØ Testing keyword: '{keyword}'\")\n",
    "    \n",
    "    # Predict category first\n",
    "    predicted_category, confidence = question_generator.predict_category(keyword)\n",
    "    print(f\"   üìÇ Predicted category: {predicted_category} (confidence: {confidence:.3f})\")\n",
    "    \n",
    "    # Generate questions\n",
    "    questions = question_generator.generate_questions(keyword, num_questions=4)\n",
    "    \n",
    "    print(f\"   üéØ Generated {len(questions)} questions:\")\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        source = q['source']\n",
    "        similarity = q['similarity']\n",
    "        print(f\"      {i}. {q['question']} [{source}, sim: {similarity:.2f}]\")\n",
    "\n",
    "print(\"\\n‚úÖ Question generation testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4a8d4",
   "metadata": {},
   "source": [
    "## üíæ B∆∞·ªõc 13: Save Trained Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Save the complete trained system\n",
    "print(\"üíæ Saving trained models and components...\")\n",
    "\n",
    "# Create a comprehensive model package\n",
    "model_package = {\n",
    "    'classifier': best_model,\n",
    "    'keyword_vectorizer': keyword_vectorizer,\n",
    "    'question_vectorizer': question_vectorizer,\n",
    "    'label_encoder': label_encoder,\n",
    "    'training_data_sample': master_dataset.sample(n=1000),  # Save sample for similarity\n",
    "    'model_info': {\n",
    "        'best_model_name': best_model_name,\n",
    "        'accuracy': best_accuracy,\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'total_training_samples': len(master_dataset),\n",
    "        'categories': CONFIG['categories']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the complete model\n",
    "model_file = MODELS_PATH / 'complete_question_ai_model.pkl'\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "print(f\"   ‚úÖ Complete model saved to: {model_file}\")\n",
    "\n",
    "# Save training results summary\n",
    "training_summary = {\n",
    "    'training_completed': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_records': len(master_dataset),\n",
    "        'unique_questions': master_dataset['question'].nunique(),\n",
    "        'unique_keywords': master_dataset['keyword'].nunique(),\n",
    "        'category_distribution': master_dataset['category'].value_counts().to_dict(),\n",
    "        'source_distribution': master_dataset['source'].value_counts().to_dict()\n",
    "    },\n",
    "    'model_performance': {\n",
    "        'best_model': best_model_name,\n",
    "        'best_accuracy': float(best_accuracy),\n",
    "        'all_model_results': {\n",
    "            name: {\n",
    "                'accuracy': float(results['accuracy']),\n",
    "                'training_time': float(results['training_time'])\n",
    "            }\n",
    "            for name, results in model_results.items()\n",
    "        }\n",
    "    },\n",
    "    'configuration': CONFIG\n",
    "}\n",
    "\n",
    "# Save training summary\n",
    "summary_file = MODELS_PATH / 'training_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"   ‚úÖ Training summary saved to: {summary_file}\")\n",
    "\n",
    "# Save sample dataset for reference\n",
    "sample_file = MODELS_PATH / 'training_data_sample.csv'\n",
    "master_dataset.sample(n=5000).to_csv(sample_file, index=False)\n",
    "print(f\"   ‚úÖ Sample training data saved to: {sample_file}\")\n",
    "\n",
    "print(\"\\nüéâ Model training and saving completed successfully!\")\n",
    "print(f\"üìä Final Results Summary:\")\n",
    "print(f\"   Best Model: {best_model_name}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   Training Samples: {len(master_dataset):,}\")\n",
    "print(f\"   Categories: {CONFIG['categories']}\")\n",
    "print(f\"   Model Files: {MODELS_PATH}\")\n",
    "\n",
    "# Create a simple test function\n",
    "def test_saved_model(keyword):\n",
    "    \"\"\"Test function to load and use the saved model\"\"\"\n",
    "    with open(model_file, 'rb') as f:\n",
    "        loaded_package = pickle.load(f)\n",
    "    \n",
    "    # Create question generator from loaded components\n",
    "    generator = AdvancedQuestionGenerator(\n",
    "        classifier=loaded_package['classifier'],\n",
    "        keyword_vectorizer=loaded_package['keyword_vectorizer'],\n",
    "        question_vectorizer=loaded_package['question_vectorizer'],\n",
    "        training_data=loaded_package['training_data_sample'],\n",
    "        label_encoder=loaded_package['label_encoder']\n",
    "    )\n",
    "    \n",
    "    return generator.generate_questions(keyword, num_questions=3)\n",
    "\n",
    "print(\"\\nüß™ Testing saved model with 'machine learning':\")\n",
    "test_result = test_saved_model(\"machine learning\")\n",
    "for i, q in enumerate(test_result, 1):\n",
    "    print(f\"   {i}. {q['question']} [{q['source']}]\")\n",
    "\n",
    "print(\"\\n‚úÖ All training completed successfully! üéä\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
